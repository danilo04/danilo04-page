---
slug: zoom-and-other-effects-compose
title: Zoom and other effects in Compose
summary: In this article, we will explore the effects such as pinch zoom and panning in Compose.
category: mobile
coverImage: /zoom-compose-cover.png
author: Danilo Dominguez
date: 2026-01-23
readTime: 10 min read
tags:
  - Quality
  - Mobile
  - Testing
  - Static analysis
  - Security
  - CI/CD
  - QA
published: true
---

In this article, we will explore the implement of effects of pinch zoom in Compose and try to understand the different APIs that we can use to achieve this. Some of the effects that we will explore are:
- Pinch zoom: is a common gesture in mobile applications. It is used to zoom in and out of a view. 
- Panning: is a common gesture in mobile applications. It is used to move a view around.

## The GraphicsLayer API

Le us introduce one component that we will use to apply the effects: the **GraphicsLayer**. GraphicsLayer is a drawing layer that records drawing commands into a display list and stores rendering properties. It isolates parts of a scene so they can be updated independently without rebuilding the whole scene. Transformations can be applied without re-recording the display list.


When drawing the display list into a destination Canvas, you can apply transformations by setting `scaleX`, `scaleY`, `translationX`, `translationY`, `rotationX`, `rotationY`, or `rotationZ`.

Imagine we have an Imagen that we want to apply a zoom effect to. We can control the zoom effect by setting the `scaleX` and `scaleY` properties.
```kotlin
Image(
    ... // other properties
    modifier = Modifier.graphicsLayer {
        scaleX = 2f
        scaleY = 2f
    }
)
```

In this example, we are setting the `scaleX` and `scaleY` properties to `2f`, which means that the image will be zoomed in by 2 times.

To evaluate how the GraphicsLayer API works, I built a simple example to show how scale and translation works. 

<Video src="/sample-app-graphics-layer.mp4" />


First, we use the `Modifier.graphicsLayer` modifier to set the `scaleX` and `scaleY` properties using values from a `Slider`. Similarly, we use a `Slider` to set the `translationX` and `translationY` properties.

```kotlin
class ZoomableState(
    initialScale: Float = 1f,
    val minScale: Float = 1f,
    val maxScale: Float = 5f
) {
     var offset by mutableStateOf(Offset.Zero)
        internal set
    ... // rest of the implementation.
}

@Composable
private fun GalleryImage(zoomableState: ZoomableState, modifier: Modifier = Modifier) {
    val id = R.drawable.happy_dog
    Surface(modifier = modifier.fillMaxSize()) {
        val context = LocalContext.current
        val model = remember(id) {
            ImageRequest.Builder(context).data(id).build()
        }

        AsyncImage(
            model = model,
            contentDescription = null,
            modifier = Modifier.graphicsLayer {
                scaleX = zoomableState.scale
                scaleY = zoomableState.scale
                translationX = zoomableState.offset.x
                translationY = zoomableState.offset.y
            },
            contentScale = ContentScale.Fit,
        )
    }
}
```

We can apply zoom effects using the `Modifier.graphicsLayer` modifier and `scaleX` and `scaleY` properties. To apply panning effects, use `translationX` and `translationY`. Modifying these properties moves the image around the screen. We want to achieve the same effect with a gesture.

Now that we have the tools to apply the effects to the images, we now focus on detecting the user gestures. 

## Detecting the user gestures

Let us try to understand some concepts before we start implementing the gesture detection:

- **Pointer**: is the object that represents the user's touch, stylus, or mouse input.
- **Pointer event**: is the event that is triggered when one or more pointers interact with the application.
- **Gesture**: is a sequence of pointer events that are detected by the system and can be interpreted as a single action.

Compose offers various modifiers for different gesture types:

| Gesture Type | Modifiers |
|--------------|-----------|
| Taps and Presses | `clickable`, `combinedClickable`, `selectable`, `toggleable`, `triStateToggleable` |
| Scrolling | `horizontalScroll`, `verticalScroll`, `scrollable` |
| Dragging | `draggable`, `swipeable` |
| Multi-touch (Panning, Rotating, Zooming) | `transformable` |

However, if we want to detect more complex gestures, we can use the `Modifier.pointerInput` modifier. This modifier allows us to detect the user gestures and apply the effects to the image.


### `pointerInput` modifier

We are going to use the `Modifier.pointerInput` modifier to detect the user gestures starting from two-finger zoom gestures. Let us try to understand how this modifier works. First, we need to understand how the `pointerInput` modifier is defined:

```kotlin
fun Modifier.pointerInput(key1: Any?, block: suspend PointerInputScope.() -> Unit): Modifier
```

or with two keys:

```kotlin
fun Modifier.pointerInput(key1: Any?, key2: Any?, block: suspend PointerInputScope.() -> Unit): Modifier
```

Notice that the `block` parameter is a suspend function is of type `PointerInputScope`. This scope provides a set of functions (some of them extension functions) to detect the user gestures. 

Here are the primary gesture detection extension functions available for PointerInputScope:

| Category | Function | Description |
|----------|----------|-------------|
| **Tap and Press Gestures** | `detectTapGestures(...)` | Detects single taps, double taps, long presses, and press events. Parameters: `onDoubleTap`, `onLongPress`, `onPress`, `onTap`. |
| | `detectTapAndPress(...)` | A variant, sometimes used in lower-level implementations to handle press and tap separation. |
| **Drag and Swipe Gestures** | `detectDragGestures(...)` | Detects drag gestures, providing callbacks for start, end, cancellation, and movement. |
| | `detectHorizontalDragGestures(...)` | Specifically detects horizontal drag movement. |
| | `detectVerticalDragGestures(...)` | Specifically detects vertical drag movement. |
| | `detectDragGesturesAfterLongPress(...)` | Initiates dragging only after a long press is detected. |
| **Multi-touch (Transform) Gestures** | `detectTransformGestures(...)` | Detects multi-touch gestures such as panning, zooming, and rotating. It provides the centroid, pan, zoom, and rotation values. |
| **Other/Custom** | `awaitEachGesture(...)` | A lower-level helper function often used to wrap other gesture detectors to ensure they restart properly after a gesture is completed. | 


There are two ways to detect the zoom gesture with two fingers. We can use the `awaitEachGesture` function or the `detectTransformGestures` function. We are going to implement both approaches.

#### `detectTransformGestures` function

The function `detectTransformGestures` is defined as follows:
```kotlin
suspend fun PointerInputScope.detectTransformGestures(
    panZoomLock: Boolean = false,
    onGesture: (centroid: Offset, pan: Offset, zoom: Float, rotation: Float) -> Unit
)
```

The function takes two parameters:
- `panZoomLock`:  if it is true, rotation is allowed only if touch slop is detected for rotation before pan or zoom motions. If not, pan and zoom gestures will be detected, but rotation gestures will not be. If panZoomLock is false, once touch slop is reached, all three gestures are detected.
- `onGesture`: a callback function that is called when the user performs a transform gesture. This callback provides with enough information to apply the zoom and panning effects to the image.

We start by implementing the `detectTransformGestures` function in the `Modifier.zoomable` modifier.

```kotlin
@Composable
fun Modifier.zoomable(
    state: ZoomableState,
    modifier: Modifier = Modifier,
    onZoomChange: ((Float) -> Unit)? = null
): Modifier {
    var scale by remember { mutableFloatStateOf(1f) }
    var offset by remember { mutableStateOf(Offset.Zero) }

    return this
        .pointerInput(state.minScale, state.maxScale) {
            detectTransformGestures(
                onGesture = { centroid, pan, gestureZoom, gestureRotate ->

                }
            )
        }.graphicsLayer {
            scaleX = scale
            scaleY = scale
            translationX = offset.x
            translationY = offset.y
        }
}
```

With this information (centroid, pan, and gestureZoom), we are ready to compute the scale and offset values to apply them to `scaleX`, `scaleY`, `translationX`, and `translationY` properties of the `graphicsLayer` modifier. To compute the scale value, we need to compute the distance between the two fingers and divide it by the initial distance between the two fingers.

Let us define a method to compute the scale value:
```kotlin
fun computeScale(gestureZoom: Float): Float {
    return (scale * gestureZoom).coerceIn(state.minScale, state.maxScale)
}
```

We compute the scale value by multiplying the current scale by the gestureZoom and coercing the result between the minimum and maximum scale values.

To compute the offset value, we just add the pan value to the current offset.
```kotlin
fun computeOffset(pan: Offset): Offset {
    return offset + pan
}
```

Let us update the `detectTransformGestures` function to use the `computeScale` and `computeOffset` methods:

```kotlin
@Composable
fun Modifier.zoomableTransformGesture(
    zoomableState: ZoomableState,
    onZoomChange: ((Float) -> Unit)? = null
): Modifier {
    var scale by remember { mutableFloatStateOf(1f) }
    var offset by remember { mutableStateOf(Offset.Zero) }

    fun computeScale(gestureZoom: Float): Float {
        return (scale * gestureZoom).coerceIn(zoomableState.minScale, zoomableState.maxScale)
    }

    fun computeOffset(pan: Offset): Offset {
        return offset + pan
    }

    return this
        .pointerInput(zoomableState.minScale, zoomableState.maxScale) {
            detectTransformGestures(
                onGesture = { _, pan, gestureZoom, _ ->
                    scale = computeScale(gestureZoom)
                    offset = computeOffset(pan)
                    onZoomChange?.invoke(scale)
                }
            )
        }.graphicsLayer {
            translationX = offset.x
            translationY = offset.y
            scaleX = scale
            scaleY = scale
        }
}
```

This implementation still has some issues. When performing a panning gesture, we move outside the image. See video below:

![Setting up touch events in Compose](/touch-events-setup.GIF)

We need to limit the panning gesture to the image boundaries. To do this, we need to compute the maximum offset value based on the image size and the current scale value. To get the image size, we use `Modifier.onSizeChange` modifier to get the size of the image. 

```kotlin
var containerSize by remember { mutableStateOf(IntSize.Zero) }

    return this
        .onSizeChanged { containerSize = it }
```

We can use the `containerSize` to compute the maximum offset value.
```kotlin
fun computeOffset(pan: Offset): Offset {
    val maxOffsetX = (scale - 1f) * containerSize.width / 2f
    val maxOffsetY = (scale - 1f) * containerSize.height / 2f

    // Multiply the scale so it panning gesture move at the same speed at any zoom level.
    val newOffset = offset + pan * scale 
    return Offset(
        newOffset.x.coerceIn(-maxOffsetX, maxOffsetX),
        newOffset.y.coerceIn(-maxOffsetY, maxOffsetY)
    )
}
```

Now, we limit the panning gesture to the image boundaries for width. For height, we still can pan outside the image into the [letter-box](https://en.wikipedia.org/wiki/Letterbox_(filming)). To avoid panning outside the image, we need to compute the maximum offset value based on the image size and the current scale value. We also need the size of the image. We are going to use the `onSuccess` callback in the `AsyncImage` to get the size of the image. 

```kotlin
var imageSize by remember { mutableStateOf(IntSize.Zero) }

AsyncImage(
    model = model,
    contentDescription = null,
    modifier = Modifier,
    contentScale = ContentScale.Fit,
    onSuccess = { result ->
        val drawable = result.result.drawable
        imageSize = IntSize(drawable.intrinsicWidth, drawable.intrinsicHeight)
    }
)
```

With the container size and the image size, we can compute the maximum offset value for width and height. 

[Image size versus container size](/zoom-image-size.png)

```kotlin
    var scale by remember { mutableFloatStateOf(1f) }
    var offset by remember { mutableStateOf(Offset.Zero) }
    var containerSize by remember { mutableStateOf(IntSize.Zero) }

    fun computeScale(gestureZoom: Float): Float {
        return (scale * gestureZoom).coerceIn(zoomableState.minScale, zoomableState.maxScale)
    }

    val w = containerSize.width.toFloat()
    val h = containerSize.height.toFloat()
    val iw = imageIntrinsicSize.width.toFloat()
    val ih = imageIntrinsicSize.height.toFloat()

    val fittedHeight = w * (ih / iw)

    val coverScaleY = h / fittedHeight 

    fun computeOffsetY(panY: Float): Float {
        // Before the image covers top+bottom, don't allow vertical pan
        if (scale <= coverScaleY) return 0f

        // Once it covers, allow pan but clamp to keep only image visible
        val maxOffsetY = (fittedHeight * scale - h) / 2f
        val newY = offset.y + panY
        return newY.coerceIn(-maxOffsetY, maxOffsetY)
    }

    fun computeOffset(pan: Offset): Offset {
        val maxOffsetX = (scale - 1f) * containerSize.width / 2f
    

        val newOffset = offset + pan * scale
        return Offset(
            newOffset.x.coerceIn(-maxOffsetX, maxOffsetX),
            computeOffsetY(pan.y)
        )
    }
```

Let me try to explain the code step by step:

- **`ContentScale.Fit`**: notice that we are using the `ContentScale.Fit` to fit the image to the container. This is important because it allows us to compute the maximum offset value for width and height.
- **`fittedHeight`** — Image height when its width fits the container: `w * (ih/iw)`. 
- **`coverScaleY`** — When you zoom with scale `s`, the image will cover the container height when `s >= coverScaleY`. We can use this value to check if we can pan vertically because the scale has covered the letter-box.
- **`computeOffsetY`** — Returns 0 when `scale <= coverScaleY` to avoid panning outside the image; otherwise clamps the vertical offset to `±(fittedHeight * scale - h) / 2` so only image is visible.

If the screen is in landscape mode, the image will be wider than the container. We need to apply similar logic to the width to avoid panning outside the image.


#### `awaitEachGesture` function

Another approach to detect the zoom and panning gestures is to use the `awaitEachGesture` function. This function allows us to detect the user gestures and apply the effects to the image.

```kotlin
suspend fun PointerInputScope.awaitEachGesture(block: suspend AwaitPointerEventScope.() -> Unit): Unit
```

The `awaitEachGesture` function takes a block as a parameter that is executed when the user interacts with the application. The block is a suspend function that takes a `AwaitPointerEventScope` as a parameter. 

We start the definition of the `Modifier.zoomable` modifier as follows:

```kotlin
@Composable 
fun Modifier.zoomableAwaitEachGesture(
    zoomableState: ZoomableState,
    imageIntrinsicSize: IntSize,
    onZoomChange: ((Float) -> Unit)? = null
): Modifier {
    ....
    
    return this
        .onSizeChanged { containerSize = it }
        .pointerInput(state.minScale, state.maxScale) {
            awaitEachGesture {
                // Wait for the initial touch.
                awaitFirstDown(requireUnconsumed = false)
        
                do {
                    // Wait for the next event.
                    val event = awaitPointerEvent()

                  // Keep processing events as long as at least one finger is down.   
                } while (event.changes.any { it.pressed }) 
            }
        }.graphicsLayer {
            scaleX = scale
            scaleY = scale
            translationX = offset.x
            translationY = offset.y
        }
}
```

Here is a video of when these events are executed: 

<div style={{ display: 'flex', justifyContent: 'center' }}>
  <img src="/touch-events-setup.GIF" alt="Touch events setup demonstration" width="400" style={{ display: 'block' }} />
</div>

<div style={{ paddingBottom: '2rem' }} />


Let us study the code step by step. First, we start by defining the `Modifier.zoomable` modifier. This modifier is a modifier that applies the zoom effect to the image. It takes a `ZoomableState` as a parameter and a `Modifier` as a parameter. The `ZoomableState` is a state object that contains the current zoom level and the minimum and maximum zoom levels. The `Modifier` is the modifier that is applied to the image.

Inside `pointerInput` modifier, we start by awaiting the initial touch. At line `11`, we use the `awaitFirstDown` function to wait for the initial touch. It doesn't block the main thread and sleeps until the user touches the screen. We don't care if the event was already consumed by another component, thus we set `requireUnconsumed` to `false`. 

Once the user touches the image, we enter the `do` loop to listen for the next events. These events are processed until the user releases the touch. We use the `awaitPointerEvent` function to wait for the next event. This function returns a `PointerEvent` object. We then process the event in the `do` loop.

To detect when the user releases the touch, we execute the `do` loop while at least one finger is down. A `PointerEvent` object has a `changes` property that contains a list of `PointerChange` objects. Each `PointerChange` object represents a change in the pointer state. For example, if the user touches the screen with two fingers, the `changes` property will contain two `PointerChange` objects. One for each finger. We check that with the `any { it.pressed }` expression which verifies if at least one finger is down.

### Detecting the zoom and panning gestures

First, we need to check the `PointerEventType` for the event. We need to check if the event is a `PointerEventType.Move`. Then, we check how many fingers are down. If there are two fingers, we can compute the zoom and pan values. If there is only one finger, we can compute the pan value.

```kotlin
@Composable 
fun Modifier.zoomableAwaitEachGesture(
    zoomableState: ZoomableState,
    imageIntrinsicSize: IntSize,
    onZoomChange: ((Float) -> Unit)? = null
): Modifier {
    ....
    
    return this
        .onSizeChanged { containerSize = it }
        .pointerInput(state.minScale, state.maxScale) {
            awaitEachGesture {
                // Wait for the initial touch.
                awaitFirstDown(requireUnconsumed = false)
        
                do {
                    // Wait for the next event.
                    val event = awaitPointerEvent()
                    if (event.type == PointerEventType.Move) {
                        if (pointerCount >= 2) {
                            // Zoom gesture
    
                            onZoomChange?.invoke(scale)
                        } else {
                            // Pan gesture
                            
                        }
                    }

                  // Keep processing events as long as at least one finger is down.   
                } while (event.changes.any { it.pressed }) 
            }
        }.graphicsLayer {
            scaleX = scale
            scaleY = scale
            translationX = offset.x
            translationY = offset.y
        }
}
```


Now, we need to compute the zoom and pan values. We can use the `computeScale` and `computeOffset` methods to compute the zoom and pan values. But first, we need to get the zoom and pan values from the event. For that we are going to use the exntesion functions `PointerEvent.calculateZoom()` and `PointerEvent.calculatePan()`.

```kotlin
@Composable 
fun Modifier.zoomableAwaitEachGesture(
    zoomableState: ZoomableState,
    imageIntrinsicSize: IntSize,
    onZoomChange: ((Float) -> Unit)? = null
): Modifier {
    ....
    
    return this
        .onSizeChanged { containerSize = it }
        .pointerInput(state.minScale, state.maxScale) {
            awaitEachGesture {
                // Wait for the initial touch.
                awaitFirstDown(requireUnconsumed = false)
        
                do {
                    // Wait for the next event.
                    val event = awaitPointerEvent()
                    if (event.type == PointerEventType.Move) {
                        if (pointerCount >= 2) {
                            // Zoom gesture
                            val gestureZoom = event.calculateZoom()
                            val pan = event.calculatePan()

                            scale = computeScale(gestureZoom)
                            offset = computeOffset(pan)
                            onZoomChange?.invoke(scale)

                            // Consume the changes
                            event.changes.forEach { change ->
                                if (change.positionChanged()) {
                                    change.consume()
                                }
                            }
                        } else {
                            // Pan gesture
                            val pan = event.calculatePan()
                            offset = computeOffset(pan)

                            // Consume the changes
                            event.changes.forEach { change ->
                                if (change.positionChanged()) {
                                    change.consume()
                                }
                            }
                        }
                    }

                  // Keep processing events as long as at least one finger is down.   
                } while (event.changes.any { it.pressed }) 
            }
        }.graphicsLayer {
            scaleX = scale
            scaleY = scale
            translationX = offset.x
            translationY = offset.y
        }
}
```

Notice the we are "consuming" the changes. This is important because it allows us to prevent the event from being processed by other components. If we don't consume the changes, the event will be processed by other components and the image might not be updated correctly. Let me explain each of the functions:

- `change.positionChanged()`: true if that pointer’s position changed in this event (e.g. the finger moved).
- `change.consume()`: Marks that pointer’s change as consumed by this modifier.

**Why consume?**

In Compose’s input system, consuming a change means: “this modifier has handled this pointer input.” Effects:
- **Stops other handlers from using the same movement**: So a parent (e.g. a vertical Scrollable or pager) or another gesture detector won’t also react to the same drag. Without consuming, both the zoomable image and the parent could move, which feels wrong.  
- **Makes it clear the gesture was “used”**: The system can treat the event as handled and not apply default behavior (e.g. scroll) to it.

So in practice: “We’ve applied this move to our zoom/pan; mark these pointer movements as consumed so nothing else (like a parent scroll) reacts to them.

## Conclusion

In this article, we have seen two approaches to detect the zoom and panning gestures in Compose. The first approach is to use the `detectTransformGestures` function. This function allows us to detect the zoom and panning gestures and apply the effects to the image. The second approach is to use the `awaitEachGesture` function. This function allows us to detect the user gestures and apply the effects to the image.

We have also seen how to compute the zoom and pan values from the event. We have also seen how to consume the changes to prevent the event from being processed by other components. This is important not just for effects but also for handling other gestures like scroll or implementing other effects.

I hope you enjoyed this article. If you have any questions, please feel free to ask me.